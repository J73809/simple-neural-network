# Simple Neural Network from Scratch

This project implements a basic 3-layer neural network using NumPy.
It uses ReLU activations in the hidden layers and Sigmoid in the output layer.
The network trains on a small sample dataset to predict multi-label outputs.

[Click here to watch the demo video](video/vizualization.mp4)

---

## Features

* Forward and backward propagation
* ReLU and Sigmoid activation functions
* Mean Squared Error loss
* Training with gradient descent

## Usage

Run the training script:

```
python main.py
```

Watch the loss and accuracy improve over 30,000 epochs.

## Requirements

* Python 3.x
* NumPy

Install dependencies with:

```
pip3 install numpy
```
